# -*- coding: utf-8 -*-
"""TransferLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eJmejAbr8BPOKLQNffJs74MbV_jok3-S

# **Transfer Learning Experiments: Generalization with CORS-ADD-HBB Data**

<p align="center">
    <img src="https://github.com/RSandAI/Comprehensive-YOLO-Airplane-Detection/blob/main/assets/image.png" height=450 width=1280 alt=""/>
</p>

<small>Picture Source: <a href="https://github.com/RSandAI/Comprehensive-YOLO-Airplane-Detection/">RSandAI, Comprehensive YOLO Airplane Detection</a></small>

<br>

## **Introduction**
Transfer learning, a technique widely employed in the domain of computer vision, involves leveraging knowledge from pre-trained models on one task to improve performance on a related task. In this experiment, we explore the efficacy of transfer learning with the CORS-ADD-HBB dataset to enhance the generalization capabilities of YOLOv8 and YOLOv9 architectures originally trained on the HRPlanes dataset.

## **Dataset Description**
The CORS-ADD-HBB dataset serves as a diverse and comprehensive testbed for evaluating model robustness and adaptability across various domains. Its curated collection of data points encompasses a wide range of scenarios, facilitating a rigorous assessment of model generalization.

## **Model Initialization**
We initialize the YOLOv8 and YOLOv9 models with weights pre-trained on the HRPlanes dataset. These pre-trained models have learned rich representations of objects within Very High Resolution (VHR) Google Earth images, providing a strong starting point for transfer learning.

## **Transfer Learning Process**

Rather than fine-tuning the entire model, we adopt a transfer learning approach where we freeze the weights of the initial layers and only update the weights of the final classification layers. This allows us to transfer knowledge from the HRPlanes dataset to the CORS-ADD-HBB dataset while minimizing the risk of overfitting.

## **Experiment Setup**
1. **Dataset Preparation**: The CORS-ADD-HBB dataset is utilized as the target domain data for transfer learning.
2. **Model Initialization**: We initialize the YOLOv8 and YOLOv9 models with pre-trained weights from the HRPlanes dataset.
3. **Transfer Learning**: The weights of the final classification layers are fine-tuned using the CORS-ADD-HBB dataset while keeping the initial layers frozen.
4. **Training**: We train the models for 5 epochs to allow them to adapt to the nuances of the new dataset.

## **Evaluation**
Post-transfer learning, we evaluate the performance of the models on the CORS-ADD-HBB test set to assess their generalization capabilities. Performance metrics such as precision, recall, and mean average precision (mAP) are computed for both YOLOv8 and YOLOv9 models.

**Top 2 YOLOv8 Models:**

| Rank | Experiment ID | Model | Network size | Optimizer | Augmentation | F1 Score | Precision | Recall | mAP50 | mAP75 | mAP50-95 |
|--|--|--|--|--|--|--|--|--|--|--|--|
| 1 | 12 | YOLOv8x | 960x960 | SGD | Hue (0.015) - Saturation (0.7) - Value (0.4) - Mosaic | 0.9932 | 0.9915 | 0.9950 | 0.9939 | 0.9925 | 0.8990 |
| 2 | 32 | YOLOv8l | 960x960 | ADAMW | Hue (0.015) - Saturation (0.7) - Value (0.4) - Mosaic | 0.9930 | 0.9927 | 0.9933 | 0.9936 | 0.9887 | 0.9025 |
| 3 | 30 | YOLOv8l | 960x960 | SGD | Hue (0.015) - Saturation (0.7) - Value (0.4) - Mosaic | 0.9922 | 0.9903 | 0.9940 | 0.9941 | 0.9917 | 0.9021 |

<br>

**Top 2 YOLOv9 Models:**

| Rank | Experiment ID | Model | Network size | Optimizer | Augmentation | F1 Score | Precision | Recall | mAP50 | mAP50-95 |
|--|--|--|--|--|--|--|--|--|--|--|
| 1 | 58 | YOLOv9e | 640x640 | SGD | Hue (0.015) - Saturation (0.7) - Value (0.4) - Mosaic | 0.9917 | 0.9918 | 0.9916 | 0.9937 | 0.8989 |
| 2 | 57 | YOLOv9e | 640x640 | SGD | None | 0.9899 | 0.9912 | 0.9886 | 0.9935 | 0.8982 |
| 3 | 62 | YOLOv9e | 640x640 | ADAMW | Hue (0.015) - Saturation (0.7) - Value (0.4) - Mosaic | 0.9899 | 0.9891 | 0.9907 | 0.9936 | 0.8930 |

<br>

Make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`.

## **0. Initial Steps**

### **0.1 Download Library**
"""

!pip install ultralytics -q

"""### **0.2. Import Libraries and Connect Google Drive**"""

from google.colab import drive
drive.mount('/gdrive')

import matplotlib.pyplot as plt
import numpy as np
import shutil
import seaborn as sns
import os
from ultralytics import YOLO

"""### **0.3. Define Paths of Model Weights**"""

# @markdown ---

# @markdown **YOLOv8 Models**

MODEL_1_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv8x/Experiment_No_12/detect/train/weights/best.pt' # @param {type:"string"}
MODEL_2_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv8l/Experiment_No_32/detect/train/weights/best.pt' # @param {type:"string"}
MODEL_3_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv8l/Experiment_No_30/detect/train/weights/best.pt' # @param {type:"string"}

# @markdown ---

# @markdown **YOLOv9 Models**

MODEL_4_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv9e/Experiment_No_58/epoch_75_100/runs/train/exp/weights/best.pt' # @param {type:"string"}
MODEL_5_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv9e/Experiment_No_57/runs/train/exp/weights/best.pt' # @param {type:"string"}
MODEL_6_PT = '/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/YOLOv9e/Experiment_No_62/epoch_75_100/runs/train/exp/weights/best.pt' # @param {type:"string"}

"""## **1. Train and Validate Models with CORS-ADD-HBB Data**

The A100 GPU is a powerful graphics processing unit (GPU) developed by NVIDIA. It is part of the NVIDIA Ampere architecture and is designed for high-performance computing tasks, including deep learning, data analytics, and scientific computing. The A100 GPU offers significant improvements in performance and efficiency compared to previous GPU models, making it ideal for demanding AI and machine learning applications.
"""

!nvidia-smi

def calculate_accuracy(TP, FN, FP):

    accuracy = (TP) / (TP + FN + FP)

    return accuracy

HOME = os.getcwd()
print(HOME)

"""### **1.1. Train and Validate Top YOLOv8 Models**

This section provides insights into the training and validation procedures applied to the top-performing YOLOv8 models utilizing the HRPlanes dataset. The models undergo training on a designated subset of the dataset, followed by validation on a distinct subset, ensuring their efficacy and reliability before further assessment.

#### **1.1.1. Train and Validate Model 1**
"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
LR = 0.001 # @param {type:"number"}
OPTIMIZER = 'SGD' # @param {type:"string"}

# @markdown ---

HUE = 0.015 # @param {type:"number"}
SATURATION = 0.7 # @param {type:"number"}
VALUE = 0.4 # @param {type:"number"}
MOSIAC = 1.0 # @param {type:"number"}

# @markdown ---

model = YOLO(MODEL_1_PT)

results = model.train(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml', epochs=20, imgsz=IMAGE_SIZE, batch=BATCH, lr0=LR, optimizer=OPTIMIZER, hsv_h=HUE, hsv_s=SATURATION, hsv_v=VALUE, mosaic=MOSIAC)

model = YOLO("runs/detect/train/weights/best.pt")
metrics = model.val(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml')

# @markdown Run this code block to monitor **box** metrics.

attributes = [
    ('metrics.box.map50', 'Mean Average Precision at IoU threshold of 0.5 for all classes.'),
    ('metrics.box.map75', 'Mean Average Precision at IoU threshold of 0.75 for all classes.'),
    ('metrics.box.ap', 'Average Precision at IoU thresholds from 0.5 to 0.95 for all classes.'),
    ('metrics.box.mp', 'Mean Precision of all classes.'),
    ('metrics.box.mr', 'Mean Recall of all classes.'),
    # ('metrics.box.mean_results', 'Mean of results, returns Mean Precision, Mean Recall, Mean Average Precision at IoU threshold of 0.5, Mean Average Precision at IoU threshold of 0.5 to 0.95.'),
    ('metrics.box.p', 'Precision for each class.'),
    ('metrics.box.r', 'Recall for each class.'),
    ('metrics.box.f1', 'F1 score for each class.'),
    ('metrics.box.nc', 'Number of classes.')
]

for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    print(f'{attr}: {value:.4f} - {explanation}')

box_metrics_dict = {}
for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    box_metrics_dict[attr] = {
        "value": value,
        "description": explanation
    }

# @markdown Run this block to monitor **best** metrics.

for key, value in metrics.results_dict.items():
    if key == 'metrics/precision(B)':
        print(f'Precision: {value:.4f} - Percentage of correct positive predictions.')
    elif key == 'metrics/recall(B)':
        print(f'Recall: {value:.4f} - Percentage of actual positives that were correctly predicted.')
    elif key == 'metrics/mAP50(B)':
        print(f'mAP50: {value:.4f} - Mean Average Precision at 50% IOU.')
    elif key == 'metrics/mAP50-95(B)':
        print(f'mAP50-95: {value:.4f} - Mean Average Precision between 50% and 95% IOU.')
    # else:
    #     print(f'{key}: {value:.4f}')

precision = metrics.results_dict['metrics/precision(B)']
recall = metrics.results_dict['metrics/recall(B)']
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'F1 Score: {f1_score:.4f} - Harmonic mean of precision and recall.')

"""Plot confusion matrix for the predictions."""

cm = metrics.confusion_matrix.matrix

plt.figure(figsize=(8, 6), dpi=300)
sns.heatmap(cm.astype(int), annot=True, fmt='d', cmap='Greys', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.xticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.yticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.show()

TP = int(cm[0][0])
FN = int(cm[1][0])
FP = int(cm[0][1])

accuracy = calculate_accuracy(TP, FN, FP)

print("Accuracy:", round(accuracy, 4))

"""#### **1.1.2. Train and Validate Model 2**"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
LR = 0.001 # @param {type:"number"}
OPTIMIZER = 'AdamW' # @param {type:"string"}

# @markdown ---

HUE = 0.015 # @param {type:"number"}
SATURATION = 0.7 # @param {type:"number"}
VALUE = 0.4 # @param {type:"number"}
MOSIAC = 1.0 # @param {type:"number"}

# @markdown ---

model = YOLO(MODEL_2_PT)

results = model.train(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml', epochs=20, imgsz=IMAGE_SIZE, batch=BATCH, lr0=LR, optimizer=OPTIMIZER, hsv_h=HUE, hsv_s=SATURATION, hsv_v=VALUE, mosaic=MOSIAC)

model = YOLO("runs/detect/train2/weights/best.pt")
metrics = model.val(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml')

# @markdown Run this code block to monitor **box** metrics.

attributes = [
    ('metrics.box.map50', 'Mean Average Precision at IoU threshold of 0.5 for all classes.'),
    ('metrics.box.map75', 'Mean Average Precision at IoU threshold of 0.75 for all classes.'),
    ('metrics.box.ap', 'Average Precision at IoU thresholds from 0.5 to 0.95 for all classes.'),
    ('metrics.box.mp', 'Mean Precision of all classes.'),
    ('metrics.box.mr', 'Mean Recall of all classes.'),
    # ('metrics.box.mean_results', 'Mean of results, returns Mean Precision, Mean Recall, Mean Average Precision at IoU threshold of 0.5, Mean Average Precision at IoU threshold of 0.5 to 0.95.'),
    ('metrics.box.p', 'Precision for each class.'),
    ('metrics.box.r', 'Recall for each class.'),
    ('metrics.box.f1', 'F1 score for each class.'),
    ('metrics.box.nc', 'Number of classes.')
]

for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    print(f'{attr}: {value:.4f} - {explanation}')

box_metrics_dict = {}
for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    box_metrics_dict[attr] = {
        "value": value,
        "description": explanation
    }

# @markdown Run this block to monitor **best** metrics.

for key, value in metrics.results_dict.items():
    if key == 'metrics/precision(B)':
        print(f'Precision: {value:.4f} - Percentage of correct positive predictions.')
    elif key == 'metrics/recall(B)':
        print(f'Recall: {value:.4f} - Percentage of actual positives that were correctly predicted.')
    elif key == 'metrics/mAP50(B)':
        print(f'mAP50: {value:.4f} - Mean Average Precision at 50% IOU.')
    elif key == 'metrics/mAP50-95(B)':
        print(f'mAP50-95: {value:.4f} - Mean Average Precision between 50% and 95% IOU.')
    # else:
    #     print(f'{key}: {value:.4f}')

precision = metrics.results_dict['metrics/precision(B)']
recall = metrics.results_dict['metrics/recall(B)']
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'F1 Score: {f1_score:.4f} - Harmonic mean of precision and recall.')

"""Plot confusion matrix for the predictions."""

cm = metrics.confusion_matrix.matrix

plt.figure(figsize=(8, 6), dpi=300)
sns.heatmap(cm.astype(int), annot=True, fmt='d', cmap='Greys', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.xticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.yticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.show()

TP = int(cm[0][0])
FN = int(cm[1][0])
FP = int(cm[0][1])

accuracy = calculate_accuracy(TP, FN, FP)

print("Accuracy:", round(accuracy, 4))



"""#### **1.1.3. Train and Validate Model 3**"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
LR = 0.001 # @param {type:"number"}
OPTIMIZER = 'SGD' # @param {type:"string"}

# @markdown ---

HUE = 0.015 # @param {type:"number"}
SATURATION = 0.7 # @param {type:"number"}
VALUE = 0.4 # @param {type:"number"}
MOSIAC = 1.0 # @param {type:"number"}

# @markdown ---

model = YOLO(MODEL_3_PT)

results = model.train(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml', epochs=20, imgsz=IMAGE_SIZE, batch=BATCH, lr0=LR, optimizer=OPTIMIZER, hsv_h=HUE, hsv_s=SATURATION, hsv_v=VALUE, mosaic=MOSIAC)

# model = YOLO("runs/detect/train3/weights/best.pt")
model = YOLO("runs/detect/train/weights/best.pt")
metrics = model.val(data='/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/coco_transfer_learning.yaml')

# @markdown Run this code block to monitor **box** metrics.

attributes = [
    ('metrics.box.map50', 'Mean Average Precision at IoU threshold of 0.5 for all classes.'),
    ('metrics.box.map75', 'Mean Average Precision at IoU threshold of 0.75 for all classes.'),
    ('metrics.box.ap', 'Average Precision at IoU thresholds from 0.5 to 0.95 for all classes.'),
    ('metrics.box.mp', 'Mean Precision of all classes.'),
    ('metrics.box.mr', 'Mean Recall of all classes.'),
    # ('metrics.box.mean_results', 'Mean of results, returns Mean Precision, Mean Recall, Mean Average Precision at IoU threshold of 0.5, Mean Average Precision at IoU threshold of 0.5 to 0.95.'),
    ('metrics.box.p', 'Precision for each class.'),
    ('metrics.box.r', 'Recall for each class.'),
    ('metrics.box.f1', 'F1 score for each class.'),
    ('metrics.box.nc', 'Number of classes.')
]

for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    print(f'{attr}: {value:.4f} - {explanation}')

box_metrics_dict = {}
for attr, explanation in attributes:
    value = getattr(metrics.box, attr.split('.')[-1])
    if isinstance(value, np.ndarray):
        value = np.float64(value[0])
    box_metrics_dict[attr] = {
        "value": value,
        "description": explanation
    }

# @markdown Run this block to monitor **best** metrics.

for key, value in metrics.results_dict.items():
    if key == 'metrics/precision(B)':
        print(f'Precision: {value:.4f} - Percentage of correct positive predictions.')
    elif key == 'metrics/recall(B)':
        print(f'Recall: {value:.4f} - Percentage of actual positives that were correctly predicted.')
    elif key == 'metrics/mAP50(B)':
        print(f'mAP50: {value:.4f} - Mean Average Precision at 50% IOU.')
    elif key == 'metrics/mAP50-95(B)':
        print(f'mAP50-95: {value:.4f} - Mean Average Precision between 50% and 95% IOU.')
    # else:
    #     print(f'{key}: {value:.4f}')

precision = metrics.results_dict['metrics/precision(B)']
recall = metrics.results_dict['metrics/recall(B)']
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'F1 Score: {f1_score:.4f} - Harmonic mean of precision and recall.')

"""Plot confusion matrix for the predictions."""

cm = metrics.confusion_matrix.matrix

plt.figure(figsize=(8, 6), dpi=300)
sns.heatmap(cm.astype(int), annot=True, fmt='d', cmap='Greys', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.xticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.yticks(ticks=[0.5, 1.5], labels=['Plane', 'Background'])
plt.show()

TP = int(cm[0][0])
FN = int(cm[1][0])
FP = int(cm[0][1])

accuracy = calculate_accuracy(TP, FN, FP)

print("Accuracy:", round(accuracy, 4))

"""### **1.2. Train and Validate Top YOLOv9 Models**

In this section, we detail the training and validation process of the top-performing YOLOv9 models using the HRPlanes dataset. The models are trained on a subset of the dataset and validated on a separate subset to ensure robust performance before further evaluation.
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/WongKinYiu/yolov9.git
# %cd yolov9
!pip install -r requirements.txt -q

"""#### **1.2.1. Train and Validate Model 4**"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
# SGD, Adam, AdamW
OPTIMIZER = 'SGD' # @param {type:"string"}

# @markdown ---

"""**IMPORTANT:** Before training, check hyperparameters in `/content/yolov9/data/hyps/hyp.scratch-high.yaml`."""

!python train_dual.py \
--batch-size {BATCH} --epochs 20 --imgsz {IMAGE_SIZE} --device 0 \
--data /content/coco_transfer_learning.yaml \
--weights {MODEL_4_PT} \
--cfg models/detect/yolov9-e.yaml \
--hyp hyp.scratch-high.yaml \
--optimizer {OPTIMIZER}

!python val_dual.py \
--img 640 --batch 16 \
--data /content/coco_transfer_learning.yaml \
--weights /content/yolov9/runs/train/exp/weights/best.pt

"""#### **1.2.2. Train and Validate Model 5**"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
# SGD, Adam, AdamW
OPTIMIZER = 'SGD' # @param {type:"string"}

# @markdown ---

"""**IMPORTANT:** Before training, check hyperparameters in `/content/yolov9/data/hyps/hyp.scratch-high.yaml`."""

!python train_dual.py \
--batch-size {BATCH} --epochs 20 --imgsz {IMAGE_SIZE} --device 0 \
--data /content/coco_transfer_learning.yaml \
--weights {MODEL_5_PT} \
--cfg models/detect/yolov9-e.yaml \
--hyp hyp.scratch-high.yaml \
--optimizer {OPTIMIZER}

!python val_dual.py \
--img 640 --batch 16 \
--data /content/coco_transfer_learning.yaml \
--weights /content/yolov9/runs/train/exp2/weights/best.pt

"""#### **1.2.3. Train and Validate Model 6**"""

# @markdown ---

IMAGE_SIZE = 640 # @param {type:"integer"}
BATCH = 16 # @param {type:"integer"}
# SGD, Adam, AdamW
OPTIMIZER = 'AdamW' # @param {type:"string"}

# @markdown ---

"""**IMPORTANT:** Before training, check hyperparameters in `/content/yolov9/data/hyps/hyp.scratch-high.yaml`."""

!python train_dual.py \
--batch-size {BATCH} --epochs 20 --imgsz {IMAGE_SIZE} --device 0 \
--data /content/coco_transfer_learning.yaml \
--weights {MODEL_6_PT} \
--cfg models/detect/yolov9-e.yaml \
--hyp hyp.scratch-high.yaml \
--optimizer {OPTIMIZER}

!python val_dual.py \
--img 640 --batch 16 \
--data /content/coco_transfer_learning.yaml \
--weights /content/yolov9/runs/train/exp3/weights/best.pt

"""## **3. Move Model Items into Google Drive**

Instead of uploading and downloading files, we can directly move them to the desired path.
"""

shutil.move("/content/runs/detect/", "/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/YOLOv8 Models/")

shutil.move("/content/yolov9/runs/", "/gdrive/MyDrive/Datasets/HRPlanes/YOLOV8/5-Transfer Learning/YOLOv9 Models/")

"""## **Conclusion**
Transfer learning emerges as a promising approach to enhance the generalization performance of object detection models. By leveraging pre-trained models and adapting them to new datasets through targeted updates, we can effectively transfer knowledge across domains. The results obtained from this experiment shed light on the real-world applicability and reliability of YOLOv8 and YOLOv9 architectures beyond their original training domains.
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")